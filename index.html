<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="nvBench 2.0: A Benchmark for Natural Language to Visualization under Ambiguity">
  <meta name="keywords" content="nvBench, NL2VIS, Natural Language, Visualization, Ambiguity">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>nvBench 2.0: A Benchmark for Natural Language to Visualization under Ambiguity</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/daillab.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  
  <style>
    .nl2vis {
      font-weight: bold;
      color: #2980B9;
    }
    .teaser img {
      width: 100%;
      max-width: 800px;
      margin: 0 auto;
    }
    .hero.is-light {
      background-color: #f4f8fd;
    }
    .hero.teaser {
      background-color: #ffffff;
    }
    .paper-figure {
      margin-bottom: 2rem;
    }
    .figure-container {
      display: flex;
      justify-content: center;
      align-items: flex-start;
      flex-wrap: wrap;
      gap: 20px;
      margin-bottom: 2rem;
    }
    .figure-vertical {
      display: flex;
      flex-direction: row;
      justify-content: space-between;
      align-items: flex-start;
      margin-bottom: 2rem;
    }
    .figure-vertical img {
      max-width: 48%;
    }
    .figure-vertical .figure-text {
      max-width: 48%;
    }
    .figure-caption {
      text-align: center;
      font-weight: bold;
      margin-top: 0.5rem;
    }
    table {
      width: 100%;
      margin-bottom: 2rem;
    }
    .tables-container {
      display: flex;
      justify-content: space-between;
      flex-wrap: wrap;
      margin-bottom: 2rem;
    }
    .table-container {
      width: 48%;
    }
    .math-formula {
      background-color: #f5f5f5;
      padding: 1rem;
      border-radius: 4px;
      margin: 1rem 0;
      overflow-x: auto;
    }
  </style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/HKUSTDial?q=nvbench&type=all&language=&sort=">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      <span>DIAL Lab</span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://nvbench2.github.io">
            nvBench 2.0
          </a>
          <a class="navbar-item" href="https://github.com/TsinghuaDatabaseGroup/nvBench">
            nvBench 1.0
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">nvBench 2.0: A Benchmark for Natural Language to Visualization under Ambiguity</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Tianqi Luo</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Chuhan Huang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Leixian Shen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#">Boyan Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Shuyu Shen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Wei Zeng</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Nan Tang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Yuyu Luo</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>HKUST(GZ),</span>
            <span class="author-block"><sup>2</sup>HKUST</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/nvBench2/nvBench2.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code & Data</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Natural Language to Visualization (nl2vis) enables users to create visualizations from natural language queries, making data insights more accessible. However, nl2vis faces challenges in interpreting ambiguous queries, as users often express their visualization needs in imprecise language.
          </p>
          <p>
            To address this challenge, we introduce nvBench 2.0, a new benchmark designed to evaluate nl2vis systems in scenarios involving ambiguous queries. nvBench 2.0 includes 7,878 natural language queries and 24,076 corresponding visualizations, derived from 780 tables across 153 domains. It is built using a controlled ambiguity-injection pipeline that generates ambiguous queries through a reverse-generation workflow. By starting with unambiguous seed visualizations and selectively injecting ambiguities, the pipeline yields multiple valid interpretations for each query, with each ambiguous query traceable to its corresponding visualization through step-wise reasoning paths.
          </p>
          <p>
            We evaluate various Large Language Models (LLMs) on their ability to perform ambiguous nl2vis tasks using nvBench 2.0. We also propose Step-nl2vis, an LLM-based model trained on nvBench 2.0, which enhances performance in ambiguous scenarios through step-wise preference optimization. Our results show that Step-nl2vis outperforms all baselines, setting a new state-of-the-art for ambiguous nl2vis tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Step-wise Disambiguation (Figure 1) -->
    <div class="figure-vertical">
      <img src="./static/images/fig1.png" alt="Example of reasoning appropriate visualizations from an ambiguous natural language query">
      <div class="figure-text">
        <h2 class="title is-3">Step-wise Disambiguation</h2>
        <div class="content">
          <p>
            When resolving ambiguities in natural language queries, we employ a step-wise reasoning approach that mimics human decision-making processes. This approach involves:
          </p>
          <ol>
            <li><strong>Data Selection Reasoning:</strong> Identifying relevant data columns and filters from the query</li>
            <li><strong>Chart Type Reasoning:</strong> Determining appropriate visualization types based on analytical tasks</li>
            <li><strong>Channel Mapping Reasoning:</strong> Assigning data elements to visual channels</li>
            <li><strong>Data Transformation Reasoning:</strong> Specifying operations like aggregation or filtering</li>
            <li><strong>Visualization Synthesis:</strong> Generating complete visualizations that represent valid interpretations</li>
          </ol>
          <p>
            This structured approach enables systematic resolution of ambiguities while preserving multiple valid interpretations of the original query.
          </p>
        </div>
      </div>
    </div>

    <!-- Ambiguity-Injected Data Synthesizer Overview (Figure 2) -->
    <div class="columns is-centered">
      <div class="column">
        <h2 class="title is-3">Ambiguity-Injected NL2VIS Data Synthesizer</h2>
        <div class="content">
          <p>
            We developed a data synthesizer that systematically introduces ambiguity into seed visualizations. This approach ensures control over the types of ambiguity while maintaining meaningful, interpretable outputs.
          </p>
        </div>
        <div class="paper-figure">
          <img src="./static/images/pipeline.png" alt="An overview of ambiguity-injected nl2vis data synthesizer">
          <p class="figure-caption">Figure 2: An overview of ambiguity-injected nl2vis data synthesizer.</p>
        </div>
      </div>
    </div>

    <!-- Ambiguity Injection Process (Figure 3) -->
    <div class="figure-vertical">
      <img src="./static/images/fig3.png" alt="Injecting ambiguities into a seed visualization">
      <div class="figure-text">
        <h2 class="title is-3">Ambiguity Injection Process</h2>
        <div class="content">
          <p>
            Our ambiguity-injection process transforms seed visualizations into ambiguity-aware visualization trees. By selectively introducing ambiguity nodes, we create multiple valid interpretations of the same query.
          </p>
          <p>
            As shown in the figure, we start with a seed chart and convert it to a visualization tree. Then, we inject ambiguities to create multiple possible interpretations. This ambiguity-aware tree can then be resolved in various ways, producing different valid visualizations for the same ambiguous query.
          </p>
          <p>
            The process ensures traceability from query to visualization through explicit reasoning paths, enabling systematic evaluation of nl2vis systems' ability to handle ambiguity.
          </p>
        </div>
      </div>
    </div>

    <!-- NL2VIS Benchmarks Comparison (Table 1) -->
    <h2 class="title is-3">Benchmark Comparison</h2>
    <div class="content">
      <p>
        nvBench 2.0 introduces several key innovations compared to existing nl2vis benchmarks, particularly its explicit handling of query ambiguity and support for one-to-many mapping between queries and visualizations.
      </p>
    </div>
    <div class="figure-container">
      <img src="./static/images/table1.png" alt="Comparison of nl2vis benchmarks">
      <p class="figure-caption">Table 1: Comparison of nl2vis benchmarks.</p>
    </div>

    <!-- NL Query Styles (Table 3) -->
    <h2 class="title is-3">Natural Language Query Distribution</h2>
    <div class="content">
      <p>
        nvBench 2.0 includes a diverse range of natural language query styles and chart types, ensuring comprehensive coverage for evaluating nl2vis systems.
      </p>
    </div>
    <div class="figure-container">
      <img src="./static/images/table3.png" alt="Distribution of natural language styles across chart types and word count statistics">
      <p class="figure-caption">Table 3: Distribution of natural language styles across chart types and word count statistics.</p>
    </div>

    <!-- Ambiguity Statistics (Tables 4 & 5) -->
    <h2 class="title is-3">Ambiguity Statistics</h2>
    <div class="content">
      <p>
        nvBench 2.0 includes detailed statistics on ambiguity types and patterns, providing insights into the distribution and frequency of different ambiguity categories.
      </p>
    </div>
    
    <div class="tables-container">
      <div class="table-container">
        <img src="./static/images/table4.png" alt="Ambiguity count at each reasoning step">
        <p class="figure-caption">Table 4: Ambiguity count at each reasoning step.</p>
      </div>
      
      <div class="table-container">
        <img src="./static/images/table5.png" alt="Statistics of ambiguity patterns">
        <p class="figure-caption">Table 5: Statistics of ambiguity patterns.</p>
      </div>
    </div>

    <!-- Step-NL2VIS Model -->
    <h2 class="title is-3">STEP-NL2VIS for Ambiguous NL2VIS</h2>
    <div class="content">
      <p>
        We propose Step-nl2vis, an LLM-based model trained on nvBench 2.0, which addresses ambiguity by incorporating a step-wise reasoning process and leveraging preference optimization.
      </p>
      
      <h3 class="title is-4">Preference Optimization with Step-DPO</h3>
      <p>
        Step-DPO utilizes step-wise paired correct and incorrect samples for preference optimization, delivering rich process supervision signals to the model and fostering improved accuracy at each step.
      </p>
      
      <div class="math-formula">
        <p>L(θ) = -E<sub>(x,s<sub>1~k-1</sub>,s<sub>win</sub>,s<sub>lose</sub>)~D<sub>p</sub></sub>[
          log σ(
            β log π<sub>θ</sub>(s<sub>win</sub>|x, s<sub>1~k-1</sub>) / π<sub>ref</sub>(s<sub>win</sub>|x, s<sub>1~k-1</sub>)
            - β log π<sub>θ</sub>(s<sub>lose</sub>|x, s<sub>1~k-1</sub>) / π<sub>ref</sub>(s<sub>lose</sub>|x, s<sub>1~k-1</sub>)
          )
        ]</p>
      </div>
      
      <p>
        Where D<sub>p</sub> represents a step-wise preference dataset, π<sub>θ</sub>(·|x, s<sub>1~k-1</sub>) denotes the policy model to be optimized, π<sub>ref</sub>(·|x, s<sub>1~k-1</sub>) refers to the reference model, and β controls the divergence between the optimized policy and the reference model.
      </p>
    </div>

    <!-- Experimental Results -->
    <h2 class="title is-3">Experiments</h2>
    <div class="content">
      <p>
        We evaluate the performance of various models on the ambiguous nl2vis task using nvBench 2.0, comparing our Step-nl2vis model against state-of-the-art approaches.
      </p>
      
      <h3 class="title is-4">Overall Performance</h3>
      <p>
        The table below presents the comprehensive performance evaluation of different models on nvBench 2.0. Our proposed Step-nl2vis achieves state-of-the-art performance across most metrics.
      </p>
    </div>
    
    <div class="figure-container">
      <img src="./static/images/table6.png" alt="Overall performance comparison between different models on nvBench 2.0">
      <p class="figure-caption">Table 6: Overall performance comparison between different models on nvBench 2.0.</p>
    </div>
    
    <div class="figure-container">
      <img src="./static/images/fig7.png" alt="F1 across different models and ambiguity levels">
      <p class="figure-caption">Figure 7: F1 across different models and ambiguity levels.</p>
    </div>
    
    <div class="figure-container">
      <img src="./static/images/fig8.png" alt="Recall across different models and ambiguity levels">
      <p class="figure-caption">Figure 8: Recall across different models and ambiguity levels.</p>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>